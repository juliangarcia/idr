{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class policy_estimator():\n",
    "    def __init__(self):\n",
    "        self.n_inputs = 2  # agent's belief, payoff\n",
    "        self.n_outputs = 1 # new belief\n",
    "\n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.ReLU())\n",
    "            #nn.Softmax(dim=-1))\n",
    "\n",
    "    def predict(self, state):\n",
    "        action = self.network(torch.FloatTensor(state))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    r = np.array([gamma**i * rewards[i]\n",
    "        for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    r = r -np.mean(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Two_Agent_Model:\n",
    "    def __init__(self, R, S, T, P, initial_belief, number_of_learning_agents, fixed_strategy):\n",
    "\n",
    "        # 0 is cooperate\n",
    "        # 1 is defect\n",
    "\n",
    "        # Game payoff matrix\n",
    "        self.game = np.array([[R, S], [T, P]])\n",
    "\n",
    "        self.payoffs = np.zeros(2, dtype=float)\n",
    "        self.fitnesses = np.zeros(2, dtype=float)\n",
    "\n",
    "        self.belief = np.full(number_of_learning_agents, initial_belief, dtype=float)\n",
    "\n",
    "        self.fixed_strategy = fixed_strategy\n",
    "        self.number_of_learning_agents = number_of_learning_agents\n",
    "\n",
    "        # Create policies for learning agents\n",
    "        self.policies = []\n",
    "        for i in range(number_of_learning_agents):\n",
    "            self.policies.append(policy_estimator())\n",
    "\n",
    "        # Create optimizers for learning agents\n",
    "        self.optimizers = []\n",
    "        for i in range(number_of_learning_agents):\n",
    "            optimizer = optim.Adam(self.policies[i].network.parameters(), lr=0.01)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "    def encounter(self, index_focal, index_other):\n",
    "\n",
    "        if self.number_of_learning_agents == 2:\n",
    "\n",
    "            # choice focal\n",
    "            choice_0_value = np.dot(self.game[0], np.array([self.belief[index_focal],\n",
    "                                                                  1.0 - self.belief[index_focal]]))\n",
    "            choice_1_value = np.dot(self.game[1], np.array([self.belief[index_focal],\n",
    "                                                                  1.0 - self.belief[index_focal]]))\n",
    "            choice_focal = 0 if choice_0_value > choice_1_value else 1\n",
    "\n",
    "            # choice other\n",
    "            choice_0_value = np.dot(self.game[0], np.array([self.belief[index_other],\n",
    "                                                                  1.0 - self.belief[index_other]]))\n",
    "            choice_1_value = np.dot(self.game[1], np.array([self.belief[index_other],\n",
    "                                                                  1.0 - self.belief[index_other]]))\n",
    "            choice_other = 0 if choice_0_value > choice_1_value else 1\n",
    "\n",
    "            return self.game[choice_focal, choice_other], self.game[choice_other, choice_focal]\n",
    "\n",
    "        else:\n",
    "            # choice focal\n",
    "            choice_0_value = np.dot(self.game[0], np.array([self.belief[index_focal],\n",
    "                                                                  1.0 - self.belief[index_focal]]))\n",
    "            choice_1_value = np.dot(self.game[1], np.array([self.belief[index_focal],\n",
    "                                                                  1.0 - self.belief[index_focal]]))\n",
    "\n",
    "            choice_focal = 0 if choice_0_value > choice_1_value else 1\n",
    "\n",
    "            choice_other = self.fixed_strategy\n",
    "\n",
    "            return self.game[choice_focal, choice_other], self.game[choice_other, choice_focal]\n",
    "\n",
    "\n",
    "    def compute_payoff(self, samples):\n",
    "        self.payoffs = np.zeros(2)\n",
    "        for _ in range(samples):\n",
    "                focal_index = 0\n",
    "                other_index = 1\n",
    "                payoff_focal, payoff_other = self.encounter(focal_index, other_index)\n",
    "                self.payoffs[focal_index] = self.payoffs[focal_index] + payoff_focal\n",
    "                self.payoffs[other_index] = self.payoffs[other_index] + payoff_other\n",
    "        self.payoffs = self.payoffs / samples\n",
    "\n",
    "    def reinforce(self, policy_estimator, number_of_episodes, number_of_steps,\n",
    "                  number_of_pretraining_episodes, number_of_pretraining_steps,\n",
    "                  batch_size, gamma, epsilon):\n",
    "        ep = 0\n",
    "\n",
    "        # Set up lists to hold results\n",
    "        total_rewards =  [ [] for _ in range(self.number_of_learning_agents)]\n",
    "        batch_rewards = [ [] for _ in range(self.number_of_learning_agents)]\n",
    "        batch_actions = [ [] for _ in range(self.number_of_learning_agents)]\n",
    "        batch_states = [ [] for _ in range(self.number_of_learning_agents)]\n",
    "        beliefs = [ [] for _ in range(self.number_of_learning_agents)]\n",
    "        for i in range(self.number_of_learning_agents):\n",
    "            beliefs[i].append(self.belief[i])\n",
    "\n",
    "        initial_belief = self.belief\n",
    "\n",
    "        # pre training\n",
    "        for i in range(number_of_pretraining_episodes):\n",
    "            states = [ [] for _ in range(self.number_of_learning_agents) ]\n",
    "            rewards = [ [] for _ in range(self.number_of_learning_agents) ]\n",
    "            actions = [ [] for _ in range(self.number_of_learning_agents) ]\n",
    "\n",
    "            for j in range(number_of_pretraining_steps):\n",
    "                # Play a round of games\n",
    "                #random_belief = np.random.uniform(size=1)[0]\n",
    "                self.compute_payoff(batch_size)\n",
    "\n",
    "                for i in range(self.number_of_learning_agents):\n",
    "                    current_state = [self.belief[i], self.payoffs[i]]\n",
    "\n",
    "                    # Select action\n",
    "                    action = self.policies[i].predict(\n",
    "                        current_state).detach().numpy()\n",
    "                    if action >= 1:\n",
    "                        action = np.array([0.99])\n",
    "                    if action <= 0:\n",
    "                        action = np.array([0.01])\n",
    "\n",
    "                    # Calculate reward\n",
    "                    self.belief[i] = action\n",
    "                    payoff_rewards = self.encounter(0, 1)\n",
    "                    r = payoff_rewards[i]\n",
    "\n",
    "                    states[i].append(current_state)\n",
    "                    rewards[i].append(r)\n",
    "                    actions[i].append(action)\n",
    "                    self.belief[i] = initial_belief[i]\n",
    "\n",
    "            # update policies\n",
    "            for i in range(self.number_of_learning_agents):\n",
    "                batch_rewards[i].extend(discount_rewards(rewards[i], gamma))\n",
    "                batch_states[i].extend(states[i])\n",
    "                batch_actions[i].extend(actions[i])\n",
    "                #total_rewards[i].append(sum(rewards[i]))\n",
    "                beliefs[i].append(self.belief[i])\n",
    "\n",
    "                optimizer = self.optimizers[i]\n",
    "                optimizer.zero_grad()\n",
    "                state_tensor = torch.FloatTensor(batch_states[i])\n",
    "                reward_tensor = torch.FloatTensor(batch_rewards[i])\n",
    "                # Actions are used as indices, must be LongTensor\n",
    "                action_tensor = torch.LongTensor(\n",
    "                    batch_actions[i])\n",
    "\n",
    "                # Calculate loss\n",
    "                logprob = torch.log(\n",
    "                    self.policies[i].predict(state_tensor))\n",
    "\n",
    "                if len(action_tensor.size()) == 1:\n",
    "                    action_tensor = action_tensor.unsqueeze(1)\n",
    "\n",
    "                selected_logprobs = reward_tensor * \\\n",
    "                                    torch.gather(logprob, 1,\n",
    "                                                 action_tensor).squeeze()\n",
    "                loss = -selected_logprobs.mean()\n",
    "\n",
    "                # Calculate gradients\n",
    "                loss.backward()\n",
    "                # Apply gradients\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_rewards[i] = []\n",
    "                batch_actions[i] = []\n",
    "                batch_states[i] = []\n",
    "\n",
    "        last_pretraining_action = action\n",
    "\n",
    "        for i in range(number_of_episodes):\n",
    "            states = [[] for _ in range(self.number_of_learning_agents)]\n",
    "            rewards = [[] for _ in range(self.number_of_learning_agents)]\n",
    "            actions = [[] for _ in range(self.number_of_learning_agents)]\n",
    "\n",
    "            for j in range(number_of_steps):\n",
    "                # Play a round of games\n",
    "                # random_belief = np.random.uniform(size=1)[0]\n",
    "                self.compute_payoff(batch_size)\n",
    "\n",
    "                for i in range(self.number_of_learning_agents):\n",
    "                    current_state = [self.belief[i], self.payoffs[i]]\n",
    "\n",
    "                    # Select action\n",
    "                    action = self.policies[i].predict(\n",
    "                        current_state).detach().numpy()\n",
    "                    if action >= 1:\n",
    "                        action = np.array([0.99])\n",
    "                    if action <= 0:\n",
    "                        action = np.array([0.01])\n",
    "\n",
    "                    # Introduce some noise\n",
    "                    x = np.random.uniform()\n",
    "                    if x < epsilon:\n",
    "                        action = np.random.uniform(size=1)\n",
    "                    if action >= 1:\n",
    "                        action = np.array([0.99])\n",
    "                    if action <= 0:\n",
    "                        action = np.array([0.01])\n",
    "\n",
    "                    # Calculate reward\n",
    "                    self.belief[i] = action\n",
    "                    payoff_rewards = self.encounter(0, 1)\n",
    "                    r = payoff_rewards[i]\n",
    "\n",
    "                    states[i].append(current_state)\n",
    "                    rewards[i].append(r)\n",
    "                    actions[i].append(action)\n",
    "\n",
    "            # update policies\n",
    "            for i in range(self.number_of_learning_agents):\n",
    "                batch_rewards[i].extend(discount_rewards(rewards[i], gamma))\n",
    "                batch_states[i].extend(states[i])\n",
    "                batch_actions[i].extend(actions[i])\n",
    "                total_rewards[i].append(sum(rewards[i]))\n",
    "                beliefs[i].append(self.belief[i])\n",
    "\n",
    "                optimizer = self.optimizers[i]\n",
    "                optimizer.zero_grad()\n",
    "                state_tensor = torch.FloatTensor(batch_states[i])\n",
    "                reward_tensor = torch.FloatTensor(batch_rewards[i])\n",
    "                # Actions are used as indices, must be LongTensor\n",
    "                action_tensor = torch.LongTensor(\n",
    "                    batch_actions[i])\n",
    "\n",
    "                # Calculate loss\n",
    "                logprob = torch.log(\n",
    "                    self.policies[i].predict(state_tensor))\n",
    "\n",
    "                if len(action_tensor.size()) == 1:\n",
    "                    action_tensor = action_tensor.unsqueeze(1)\n",
    "\n",
    "                selected_logprobs = reward_tensor * \\\n",
    "                                    torch.gather(logprob, 1,\n",
    "                                                 action_tensor).squeeze()\n",
    "                loss = -selected_logprobs.mean()\n",
    "\n",
    "                # Calculate gradients\n",
    "                loss.backward()\n",
    "                # Apply gradients\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_rewards[i] = []\n",
    "                batch_actions[i] = []\n",
    "                batch_states[i] = []\n",
    "\n",
    "                if (ep+1) % 10 == 0:\n",
    "                    print('\\rEpisode {}\\tAverage reward of last 100: {:.2f}'.format(ep+1, avg_rewards))\n",
    "                    print(\"Current belief \", self.belief[0])\n",
    "                ep += 1\n",
    "\n",
    "            actual_rewards = np.array(total_rewards)/number_of_steps\n",
    "            return actual_rewards, beliefs, last_pretraining_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_results(beliefs):\n",
    "    plt.plot(beliefs)\n",
    "    plt.xlabel('Episode');\n",
    "    plt.ylabel('Beliefs')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    count_nonzero = 0\n",
    "    count_zero = 0\n",
    "    mc_beleifs1 = np.zeros(102, dtype=float)\n",
    "    mc_beleifs_nonzero = np.zeros(102, dtype=float)\n",
    "    for i in range(10):\n",
    "        model = Two_Agent_Model(4, 1, 3, 2, 0.9, 1, 0)\n",
    "        policy_est = policy_estimator()\n",
    "        rewards, beliefs, last_pretraining_action = model.reinforce(policy_est, 100, 1000, 100, 1000, 10, 0.99, 0.00)\n",
    "        beliefs = beliefs[0]\n",
    "        if last_pretraining_action == 0.01:\n",
    "            count_zero += 1\n",
    "        else:\n",
    "            count_nonzero += 1\n",
    "            for j in range(len(beliefs)):\n",
    "                mc_beleifs_nonzero[j] = mc_beleifs_nonzero[j] + beliefs[j]\n",
    "        for j in range(len(beliefs)):\n",
    "            mc_beleifs1[j] = mc_beleifs1[j] + beliefs[j]\n",
    "    mc_beleifs1 = mc_beleifs1/10\n",
    "    mc_beleifs_nonzero = mc_beleifs_nonzero/count_nonzero\n",
    "\n",
    "    mc_beleifs2 = np.zeros(102, dtype=float)\n",
    "    for i in range(10):\n",
    "        model = Two_Agent_Model(4, 1, 3, 2, 0.9, 1, 1)\n",
    "        policy_est = policy_estimator()\n",
    "        rewards, beliefs, last_pretraining_action = model.reinforce(policy_est, 100, 100, 100, 100, 10, 0.99, 0.00)\n",
    "        beliefs = beliefs[0]\n",
    "        for j in range(len(beliefs)):\n",
    "            mc_beleifs2[j] = mc_beleifs2[j] + beliefs[j]\n",
    "    mc_beleifs2 = mc_beleifs2/10\n",
    "\n",
    "    mc_beleifs3 = np.zeros(102, dtype=float)\n",
    "    mc_beleifs4 = np.zeros(102, dtype=float)\n",
    "    for i in range(10):\n",
    "        model = Two_Agent_Model(4, 1, 3, 2, 0.9, 2, 1)\n",
    "        policy_est = policy_estimator()\n",
    "        rewards, beliefs, last_pretraining_action = model.reinforce(policy_est, 100, 1000, 100, 1000, 10, 0.99, 0.00)\n",
    "        for j in range(len(beliefs[0])):\n",
    "            mc_beleifs3[j] = mc_beleifs3[j] + beliefs[0][j]\n",
    "        for j in range(len(beliefs[1])):\n",
    "            mc_beleifs4[j] = mc_beleifs4[j] + beliefs[1][j]\n",
    "    mc_beleifs3 = mc_beleifs3/10\n",
    "    mc_beleifs4 = mc_beleifs4/10\n",
    "\n",
    "    print(\"1 learning agent, 1 dummy agent with fixed startegy 0\")\n",
    "    plot_results(mc_beleifs1)\n",
    "    print(\"Number of non-defective mc runs: \", str(count_nonzero))\n",
    "    print(\"Just non-defective mc runs\")\n",
    "    plot_results(mc_beleifs_nonzero)\n",
    "    print(\"1 learning agent, 1 dummy agent with fixed startegy 1\")\n",
    "    plot_results(mc_beleifs2)\n",
    "    print(\"2 learning agents\")\n",
    "    plot_results(mc_beleifs3)\n",
    "    plot_results(mc_beleifs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def main(config_file_path):\n",
    "    with open(config_file_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    model = Model(config[\"number_of_agents\"],\n",
    "                  config[\"R\"], config[\"S\"],\n",
    "                  config[\"T\"], config[\"P\"],\n",
    "                  config[\"tag0_initial_ingroup_belief\"],\n",
    "                  config[\"tag0_initial_outgroup_belief\"],\n",
    "                  config[\"tag1_initial_ingroup_belief\"],\n",
    "                  config[\"tag1_initial_outgroup_belief\"],\n",
    "                  config[\"initial_number_of_0_tags\"])\n",
    "    model.run_simulation(config[\"random_seed\"], config[\"number_of_steps\"],\n",
    "                         config[\"rounds_per_step\"],\n",
    "                         config[\"selection_intensity\"],\n",
    "                         config[\"perturbation_probability\"],\n",
    "                         config[\"perturbation_scale\"], config[\"data_recording\"],\n",
    "                         config[\"data_file_path\"], config[\"write_frequency\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
