\documentclass[]{llncs}
\usepackage{graphicx}

%\documentclass{article}
\usepackage{mathtools}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{arydshln}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{epstopdf}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{rotating}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
%\usepackage[algo2e]{algorithm2e}
%\usepackage{arevmath}     % For math symbols
\usepackage[noend]{algpseudocode}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE T
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\pagestyle{plain}
\setcounter{page}{1}
\pagenumbering{arabic}

\DeclareMathOperator {\argmax}{argmax} 

\begin{document}

\title{Adaptive Dynamics for the Direct Actions Strategy Approach}

\author{Frances Cameron-Muller, Supervisor: Dr. Julian Garcia}

\institute{Monash University}

\maketitle    

\section{Adaptive Dynamics for Once Off Stag Hunt Game}

Let $x_i \in [0,1]$ be the probability that agent i cooperates. \\
The Stag Hunt Payoff matrix is 
\[
  \begin{pmatrix} 
   R & S  \\
   T & P  
   \end{pmatrix} 
\]
with $ R > T \geq P > S $ \\
\\
Let $ \Pi ( y, x_{-} )$ be the payoff function of playing a strategy y against the rest of the population playing the strategy x. 
\[
\Pi ( y, x_{-} ) = R x y + S (1-x) y +  T x (1-y) + P (1-x) (1-y)
\]
\\
The invasion fitness of a rare mutant strategy y in a population that consists of only strategy x is given by 
\[
f_x(y) = \Pi ( y, x_{-} ) - \Pi ( x, x_{-} )
\]
\begin{multline}
f_x(y) = R x y + S (1-x) y +  T x (1-y) + P (1-x) (1-y) \\ - R x^2 - S (1-x) x - T x (1-x) - P (1-x) (1-x)
\end{multline}
\[
D(x) = \diffp*{f_x(y)}{y}{y=x}
\]
\[
D(x) = (R - S - T +P) x + (S-P)
\]
\\
Let $H = \frac{P-S}{R - S - T +P} $ also known as the threshold with $ D(H) = 0 $.\\
\\
For $x < H, D(x) < 0$ and for $x > H, D(x) > 0$
This means that for x values below the threshold, the best payoffs are achieved by decreasing the value of x and converging to the Nash equilibrium of always defecting and for x values above the threshold, the best payoffs are achieved by increasing the value of x and converging to the Nash equilibrium of always cooperating. 

\subsection{Example}

Consider a model with dominant strategy $x = 0.1$ interpreted as agents will only cooperate in 10\% of games and a rare mutant strategy $y = 0.7$ representing agents will cooperate in 70\% of games. 
\[
   \text{Payoff Matrix} = \begin{pmatrix} 
   6 & 1  \\
   3 & 2  
   \end{pmatrix} 
\]
The invasion fitness of the rare mutant strategy y = 0.7 is 
\begin{multline}
f_{0.1}(0.7) = 6 0.1 0.7 + 1 (1-0.1) 0.7 +  3 0.1 (1-0.7) + 2 (1-0.1) (1-0.7) \\ - 6 0.1^2 - 1 (1-0.1) 0.1 - 3 0.1 (1-0.1) - 1 (1-0.1) (1-0.1) = 0.45
\end{multline}
The strategy has a positive invasion fitness.
\[
D(x) = D(0.1) = (6 - 1 - 3 +2) 0.1 + (1-2) = -0.6
\]
Since the partial derivative is negative this suggests that the current dominant belief of $x = 0.1$ should be increased. 


\section{Adaptive Dynamics for Repeated Stag Hunt Games}

Now let's introduce a new parameter $\alpha \in [0, 1]$ where $\alpha$ is the probability that an agent is matched to play against a copy of themselves instead of their original opponent. 
\\
Therefore, the expected payoff is given by the sum of payoffs of when an agent plays their copy and when an agent plays their original opponent, weighted by the probabilities.
\[ 
\Pi ( y, x_{-}, \alpha ) = \alpha \Pi ( y, y) + (1-\alpha) \Pi ( y, x_{-} )
\]
The new invasion fitness of a rare mutant strategy y in a population that consists of only strategy x is given by 
\[
f_x(y) = \Pi ( y, x_{-}, \alpha ) - \Pi ( x, x_{-}, \alpha )
\]
\[
f_x(y) = \alpha \Pi ( y, y) + (1-\alpha) \Pi ( y, x_{-} ) - \alpha \Pi ( x, x) - (1-\alpha) \Pi ( x,  x_{-} )
\]
\[
f_x(y) = \alpha \Pi ( y, y) + (1-\alpha) \Pi ( y, x_{-} ) - \Pi ( x,  x_{-} )
\]
\begin{multline}
f_x(y) = \alpha (R-S-T+P) y^2 +(S + \alpha T - P - \alpha P) y + (1-\alpha) (R-S-T+P) x y \\+ (-\alpha T + \alpha P - S + P) x + (-R+S+T-P) x^2
\end{multline}
\[
D(x) = \diffp*{f_x(y)}{y}{y=x}
\]
\[
D(x) = (1+\alpha) (R-S-T+P) x + (S + \alpha T - P - \alpha P) 
\]
\[
H = \frac{S + \alpha T - P - \alpha P}{(1+\alpha)(R-S-T+P)}
\]

\section{Graphs}
The derivative of the invasion fitness function of a rare mutant strategy, D(x), is graphed below for all possible dominant strategies, x $\in [0,1]$. Negative values of D(x) indicate the best payoffs are achieved by decreasing the value of x whereas for positive values of D(x), greater payoffs are achieved by increasing x. The function D(x) was graphed for two different payoff matrices [4,1:3,2] and [6,1;3,2]. This shows how the invasion fitness of a rare mutant strategy is impacted by the relative difference in the payoffs between cooperating and defecting. The graph below shows that when the payoff for cooperating is much greater than the payoff for cooperating, the threshold at which D(x) changes sign occurs at a smaller value of x. This demonstrates that greater relative payoffs for cooperation increase the probability of social cooperation as rare mutant strategies are more likely to increase the dominant strategy. This behaviour was observed in the simulation results. 

\begin{figure}
\centering
\includegraphics[width=12cm]{images/invasion_fitness}
\end{figure}

The second graph shows the same function as the graph above, the derivative of the invasion fitness function of a rare mutant strategy, D(x), when we incorporate the parameter, alpha. The graphs shows D(x) for the same payoff matrix [4,1;3,2] but for three values of alpha. When alpha = 0, meaning the  probability that an agent is matched to play against a copy of themselves instead of their original opponent is zero, we observe the exact same function in the previous graph, as expected. When alpha = 1, D(x) is always positive illustrating that the agents learning to cooperate is always achieved if the agents are only playing against a copy of themselves. For alpha values between 0 and 1, as alpha increases the threshold for D(x) decreases. Therefore, the more likely the agents are to play against opponents that are similar to themselves, the more likely they will learn to cooperate. 

\begin{figure}
\centering
\includegraphics[width=12cm]{images/invasion_fitness_alpha}
\end{figure}


\end{document}
